


	Graph consists of either:
	
		SmerMap (indexing, optimized for adding nodes)
		SmerArray (routing)
		
	SmerMap
		16384 slices (2^14) - balanced using prefix-mixing
		
		Node split into 7bp prefix and lower 16bp
	
		Hash calculated based on the lower 16bp
			Lower part of hash determined location in slice
			Upper part of hash mixed with 7bp prefix determines slice
			
			 
	SmerArray
		16384 slices (2^14) - 1:1 with SmerMap slices
		
		Each slice contains queue of inbound requests from other slices
		
		
		
	Profiles:
		
		export CPUPROFILE=gperf.out
	
		pprof LOGAN-Graph-Native/bin/LOGAN gperf.out
		 
		top20
	
		
	Dataset Profiles:		
		
		Ecoli-1: Reads: 7248696 Smers: 1810392 (single threaded)


		 7.378805s using 8 threads (desktop) 
		 
		 
    2315  50.9%  50.9%     2315  50.9% siitFindSmer
     559  12.3%  63.2%      569  12.5% scanForSmer_HS (inline)
     337   7.4%  70.6%     2785  61.2% saFindSmer (inline)
     293   6.4%  77.1%      297   6.5% readBufferedFastqLine (inline)
     174   3.8%  80.9%      198   4.4% calculatePossibleSmersComp
     168   3.7%  84.6%      175   3.8% skipBufferedFastqLine (inline)
      83   1.8%  86.4%       87   1.9% hashForSmer
      80   1.8%  88.2%       80   1.8% packSequence
      70   1.5%  89.7%     2818  62.0% saFindIndexesOfExistingSmers
      45   1.0%  90.7%       61   1.3% calculatePossibleSmers
      45   1.0%  91.7%       45   1.0% smGetSmerCount_S (inline)
      43   0.9%  92.6%       43   0.9% __read_nocancel
      40   0.9%  93.5%      651  14.3% smFindIndexesOfExistingSmers
      36   0.8%  94.3%       36   0.8% sliceForSmer
      33   0.7%  95.0%      594  13.1% findSmer_HS (inline)
      32   0.7%  95.7%     3112  68.4% trDoIngress
      29   0.6%  96.4%      761  16.7% addPathSmers
      25   0.5%  96.9%       33   0.7% reverseWithinBytes (inline)
      24   0.5%  97.4%       24   0.5% __strchr_sse42
      19   0.4%  97.9%      559  12.3% parseAndProcess
		
	
		Ecoli-2: Reads: 218470592 Smers: 5717584
		
		
		374.014427s using 16 threads (cessna)
	
	
  144137  58.9%  58.9%   144148  59.0% siitFindSmer
   20208   8.3%  67.2%    20986   8.6% scanForSmer_HS (inline)
   14070   5.8%  73.0%    14166   5.8% readBufferedFastqLine (inline)
   12889   5.3%  78.2%   164514  67.3% saFindSmer (inline)
    7587   3.1%  81.3%     8279   3.4% calculatePossibleSmersComp
    7180   2.9%  84.3%     7405   3.0% skipBufferedFastqLine (inline)
    5089   2.1%  86.4%     5093   2.1% __read_nocancel
    4153   1.7%  88.1%     4193   1.7% hashForSmer
    4128   1.7%  89.7%   166464  68.1% saFindIndexesOfExistingSmers
    3719   1.5%  91.3%     4471   1.8% calculatePossibleSmers
    3461   1.4%  92.7%     3461   1.4% packSequence
    2368   1.0%  93.7%    23089   9.4% findSmer_HS (inline)
    2203   0.9%  94.6%     2203   0.9% sliceForSmer
    2022   0.8%  95.4%    25632  10.5% smFindIndexesOfExistingSmers
    1945   0.8%  96.2%    33247  13.6% addPathSmers
    1192   0.5%  96.7%     1436   0.6% reverseWithinBytes (inline)
    1184   0.5%  97.1%     1308   0.5% checkStealthIndexing (inline)
    1090   0.4%  97.6%   178321  72.9% trDoIngress
     927   0.4%  98.0%      927   0.4% __strchr_sse42
     781   0.3%  98.3%      781   0.3% __lll_unlock_wake
	
	
	
		Arabi-1: Reads: 29493880 Smers: 7969993 (single threaded)
	
		34.352455s using 16 threads (cessna)
		
   18525  55.9%  55.9%    18527  55.9% siitFindSmer
    4276  12.9%  68.8%     4449  13.4% scanForSmer_HS (inline)
    1695   5.1%  73.9%     1707   5.1% readBufferedFastqLine (inline)
    1418   4.3%  78.2%    20812  62.8% saFindSmer (inline)
     977   2.9%  81.1%     1001   3.0% skipBufferedFastqLine (inline)
     860   2.6%  83.7%      956   2.9% calculatePossibleSmersComp
     567   1.7%  85.4%      576   1.7% hashForSmer
     445   1.3%  86.7%    21009  63.4% saFindIndexesOfExistingSmers
     424   1.3%  88.0%      519   1.6% calculatePossibleSmers
     419   1.3%  89.3%     4771  14.4% findSmer_HS (inline)
     389   1.2%  90.5%     5292  16.0% smFindIndexesOfExistingSmers
     383   1.2%  91.6%      383   1.2% packSequence
     326   1.0%  92.6%      326   1.0% __read_nocancel
     293   0.9%  93.5%      293   0.9% __lll_unlock_wake
     287   0.9%  94.3%      289   0.9% sliceForSmer
     261   0.8%  95.1%     6301  19.0% addPathSmers
     232   0.7%  95.8%      232   0.7% __lll_lock_wait
     168   0.5%  96.3%      196   0.6% checkStealthIndexing (inline)
     151   0.5%  96.8%      188   0.6% reverseWithinBytes (inline)
     145   0.4%  97.2%      145   0.4% __strchr_sse42
		
	
	
		Pen-1: Reads: 450786851 reads Smers: 54666725
		
		944.455062s using 16 threads (cessna)
		
		
  955612  76.7%  76.7%   955659  76.7% siitFindSmer
   93871   7.5%  84.2%    97351   7.8% scanForSmer_HS (inline)
   41190   3.3%  87.5%    41326   3.3% readBufferedFastqLine (inline)
   24229   1.9%  89.5%  1003215  80.5% saFindSmer (inline)
   22859   1.8%  91.3%    24940   2.0% calculatePossibleSmersComp
   13306   1.1%  92.4%    13408   1.1% hashForSmer
   13233   1.1%  93.4%  1009389  81.0% saFindIndexesOfExistingSmers
   11134   0.9%  94.3%    13204   1.1% calculatePossibleSmers
   10065   0.8%  95.1%    10065   0.8% packSequence
    8593   0.7%  95.8%   104481   8.4% findSmer_HS (inline)
    7090   0.6%  96.4%     7094   0.6% sliceForSmer
    7017   0.6%  97.0%   113392   9.1% smFindIndexesOfExistingSmers
    6581   0.5%  97.5%   138362  11.1% addPathSmers
    5004   0.4%  97.9%     5010   0.4% __read_nocancel
    4933   0.4%  98.3%     5404   0.4% checkStealthIndexing (inline)
    3482   0.3%  98.6%     4257   0.3% reverseWithinBytes (inline)
    2413   0.2%  98.8%     2413   0.2% __strchr_sse42
    2301   0.2%  98.9%     2432   0.2% skipBufferedFastqLine (inline)
    1734   0.1%  99.1%     1734   0.1% madvise
    1717   0.1%  99.2%     1720   0.1% smGetSmerCount_S (inline)


	Pack Kmers

A	0x41	65		0100 0001
C	0x43	67		0100 0011
G	0x47	71		0100 0111
T	0x54	84		0101 0100

00 00
01 00
11 01
10 01





	Calculate Possible Kmers:


	
	Pack -> Smer
		
	Rmer:             -- -- -- --  -- -- -- --  23 22 21 20  19 18 17 16  15 14 13 12  11 10 09 08  07 06 05 04  03 02 01 00
	
	Fmer=bswap64(Rmer)
	  
	Fmer:             00 01 02 03  04 05 06 07  08 09 10 11  12 13 14 15  16 17 18 19  20 21 22 23  -- -- -- --  -- -- -- --
	
	Rmer = Rmer << 2bp 
	Fmer = Fmer >> 8bp
	
	
	Fmer1:            00 01 02 03  04 05 06 07  08 09 10 11  12 13 14 15  16 17 18 19  20 21 22 23   
	
	FmerA: (>>1bp)       00 01 02  03 04 05 06  07 08 09 10  11 12 13 14  15 16 17 18  19 20 21 22     
	FmerB:               01 02 03  04 05 06 07  08 09 10 11  12 13 14 15  16 17 18 19  20 21 22 23
	
	Fmer2:  01 02 03  04 05 06 07  08 09 10 11  12 13 14 15  16 17 18 19  20 21 22 23  24*25*26*27*
	
	FmerC: (>>3bp)       02 03 04  05 06 07 08  09 10 11 12  13 14 15 16  17 18 19 20  21 22 23 24*
	FmerD: (>>2bp)       03 04 05  06 07 08 09  10 11 12 13  14 15 16 17  18 19 20 21  22 23 24*25*

	FmerA2: (>>1bp)      04 04 06  07 08 09 10  11 12 13 14  15 16 17 18  19 20 21 22  23 24*25*26*
	FmerB2:              05 06 07  08 09 10 11  12 13 14 15  16 17 18 19  20 21 22 23  24*25*26*27*


	Rmer1:     23 22  21 20 19 18  17 16 15 14  13 12 11 10  09 08 07 06  05 04 03 02  01 00 -- --  

	RmerA: (>>2bp)       22 21 20  19 18 17 16  15 14 13 12  11 10 09 08  07 06 05 04  03 02 01 00     
	RmerB: (>>3bp)       23 22 21  20 19 18 17  16 15 14 13  12 11 10 09  08 07 06 05  04 03 02 01
	
	Rmer2:    27*26*  25*24*23 22  21 20 19 18  17 16 15 14  13 12 11 10  09 08 07 06  05 04 03 02
	
	RmerC:               24*23 22  21 20 19 18  17 16 15 14  13 12 11 10  09 08 07 06  05 04 03 02 
	RmerD: (>>1bp)       25*24*23  22 21 20 19  18 17 16 15  14 13 12 11  10 09 08 07  06 05 04 03 

	RmerA2: (>>2bp)      26*25*24* 23 22 21 20  19 18 17 16  15 14 13 12  11 10 09 08  07 06 05 04 
	RmerB2: (>>3bp)      27*26*25* 24 23 22 21  20 19 18 17  16 15 14 13  12 11 10 09  08 07 06 05

								

								
	Cache Structure:
		32K / 256K / 2-2.5M per core (sandy/ivy/haswell)
																										
	Bloom filters: Option to speed up SmerArray queries
		Expect true positive rate 10-20%	
		4-8 bits per entry for 2-15% false positive rate
		
		Pen D dataset:
			Single: 50M smers * 4bits -> 25MB
			Mid: 384K smers * 4 bits -> 192KB
			Slice: 3K smers * 4 bits -> 1.5KB
			
		Notional dataset (100x pen D):
			Single: 5000M smers * 4 bits -> 2500MB
			Mid: 38.5M smers * 4 bits -> 19.2MB
			Slice: 300K smers * 4 bits -> 150KB
		
		Consider blocked bloom filters: All queries for a given key hit the same 64 byte region
			Pattern filters also possible (SIMD advantages)
		
	Read batching: 
		Currently 1024 reads per batch -> ~80 kmers per read -> 80000 kmers per batch (~5 per slice). 640K per batch
		10240 reads per batch -> ~80 kmers per read -> 800000 kmers per batch (~50 per slice). 6.4M per batch			

	Sub-bloom filters:
		Idea: Build one or more 'read batch specific' bloom filters from the smers within a read batch
		Improves cache behaviour
		
		 
				
		
		
Arabi-1 Dataset: Seq: 30361968 Bp: 2364495967


Kmers: 154717256
Kmers occuring once: 42518318 2434485 1976988 2988053 4500337

1696347756 occurences of Kmers in reads

4004635		0.2360739% in top 10	394418
35389971	2.086245% in top 100	309504
82001337	4.833993% in top 1k		17894
167078291	9.849295% in top 10k	6549
285329471	16.82022% in top 100k	551
465014713	27.4127% in top 1m		57
656224623	38.68456% in top 10m	17
1606781157	94.72003% in top 100m	6


Smers: 7970136
Smers occuring <5: 1394227 103341 109399 178212 270757

220840462 occurences of Smers in reads

3963627		 1.794792% in top 10		390927
34285440	15,52498% in top 100		264374
67480501	30.55622% in top 1k			13637
115890440	52.477% in top 10k			1371
146946106	66.53948% in top 100k		110
166856524	75.55523% in top 1m			15





Pen-1 Dataset:



Smers: 53836270
LOGAN INFO cliSrc/fastqParser.c(449) 1289.080710: Used 450786851 450786851
LOGAN INFO cliSrc/main.c(260) 1289.080796: Indexing: Parsed 450786851 reads from data/Pen-1/D.fq
LOGAN INFO src/task/task.c(93) 1289.080803: Master: QueueShutdown
LOGAN INFO src/task/task.c(102) 1289.080807: Master: QueueShutdown locked
LOGAN INFO src/task/task.c(110) 1289.080811: Master: QueueShutdown Complete
LOGAN INFO src/task/task.c(37) 1289.080816: Master: Waiting for shutdown
LOGAN INFO src/task/task.c(227) 1289.347379: New Shutdown Req
LOGAN INFO src/task/task.c(396) 1289.347405: Worker 0 Completed main loop - waiting for shutdown tidy
LOGAN INFO src/task/taskIndexing.c(20) 1289.347413: TaskIndexing: DoDeregister (0)
LOGAN INFO src/task/task.c(442) 1289.347418: Worker 0 last out, notifying master
LOGAN INFO src/task/task.c(51) 1289.347481: Master: Done Shutdown
LOGAN INFO cliSrc/main.c(436) 1289.513146: Smer count: 54666725


























/* gprof-helper.c -- preload library to profile pthread-enabled programs
 *
 * Authors: Sam Hocevar <sam at zoy dot org>
 *          Daniel JÃ¶nsson <danieljo at fagotten dot org>
 *
 *  This program is free software; you can redistribute it and/or
 *  modify it under the terms of the Do What The Fuck You Want To
 *  Public License as published by Banlu Kemiyatorn. See
 *  http://sam.zoy.org/projects/COPYING.WTFPL for more details.
 *
 * Compilation example:
 * gcc -shared -fPIC gprof-helper.c -o gprof-helper.so -lpthread -ldl
 *
 * Usage example:
 * LD_PRELOAD=./gprof-helper.so your_program
 */

#define _GNU_SOURCE
#include <sys/time.h>
#include <stdio.h>
#include <stdlib.h>
#include <dlfcn.h>
#include <pthread.h>

static void * wrapper_routine(void *);

/* Original pthread function */
static int (*pthread_create_orig)(pthread_t *__restrict,
                                  __const pthread_attr_t *__restrict,
                                  void *(*)(void *),
                                  void *__restrict) = NULL;

/* Library initialization function */
void wooinit(void) __attribute__((constructor));

void wooinit(void)
{
    pthread_create_orig = dlsym(RTLD_NEXT, "pthread_create");
    fprintf(stderr, "pthreads: using profiling hooks for gprof\n");
    if(pthread_create_orig == NULL)
    {
        char *error = dlerror();
        if(error == NULL)
        {
            error = "pthread_create is NULL";
        }
        fprintf(stderr, "%s\n", error);
        exit(EXIT_FAILURE);
    }
}

/* Our data structure passed to the wrapper */
typedef struct wrapper_s
{
    void * (*start_routine)(void *);
    void * arg;

    pthread_mutex_t lock;
    pthread_cond_t  wait;

    struct itimerval itimer;

} wrapper_t;

/* The wrapper function in charge for setting the itimer value */
static void * wrapper_routine(void * data)
{
    /* Put user data in thread-local variables */
    void * (*start_routine)(void *) = ((wrapper_t*)data)->start_routine;
    void * arg = ((wrapper_t*)data)->arg;

    /* Set the profile timer value */
    setitimer(ITIMER_PROF, &((wrapper_t*)data)->itimer, NULL);

    /* Tell the calling thread that we don't need its data anymore */
    pthread_mutex_lock(&((wrapper_t*)data)->lock);
    pthread_cond_signal(&((wrapper_t*)data)->wait);
    pthread_mutex_unlock(&((wrapper_t*)data)->lock);

    /* Call the real function */
    return start_routine(arg);
}

/* Our wrapper function for the real pthread_create() */
int pthread_create(pthread_t *__restrict thread,
                   __const pthread_attr_t *__restrict attr,
                   void * (*start_routine)(void *),
                   void *__restrict arg)
{
    wrapper_t wrapper_data;
    int i_return;

    /* Initialize the wrapper structure */
    wrapper_data.start_routine = start_routine;
    wrapper_data.arg = arg;
    getitimer(ITIMER_PROF, &wrapper_data.itimer);
    pthread_cond_init(&wrapper_data.wait, NULL);
    pthread_mutex_init(&wrapper_data.lock, NULL);
    pthread_mutex_lock(&wrapper_data.lock);

    /* The real pthread_create call */
    i_return = pthread_create_orig(thread,
                                   attr,
                                   &wrapper_routine,
                                   &wrapper_data);

    /* If the thread was successfully spawned, wait for the data
     * to be released */
    if(i_return == 0)
    {
        pthread_cond_wait(&wrapper_data.wait, &wrapper_data.lock);
    }

    pthread_mutex_unlock(&wrapper_data.lock);
    pthread_mutex_destroy(&wrapper_data.lock);
    pthread_cond_destroy(&wrapper_data.wait);

    return i_return;
}



